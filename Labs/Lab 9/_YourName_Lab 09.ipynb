{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f26ddd3",
   "metadata": {},
   "source": [
    "\n",
    "# Lab 9: Build a Log Aggregator\n",
    "\n",
    "In this lab, you will create your own log generator, build a command-line utility that scans log files, summarizes their contents, and provides insight into system behavior. Data structures to track log message levels such as `INFO`, `WARNING`, `ERROR`, and `CRITICAL`.\n",
    "\n",
    "This lab reinforces:\n",
    "- File I/O\n",
    "- Pattern recognition (regex)\n",
    "- Dictionaries and counters\n",
    "- Functions and modularity\n",
    "- CLI arguments, logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d5ee8a",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Create Log files (20%)\n",
    "Using the the following example log format below create a **python file** that will log errors In a structured tree format \n",
    "\n",
    "You will find examples in the folder called Logs that you can use to build your program.\n",
    "\n",
    "Remember set of logs should have a varied levels of log entries (`INFO`, `WARNING`, `ERROR`, `CRITICAL`) and tailored message types for different service components.\n",
    "You must create 5 structured logs here are some examples:\n",
    "\n",
    "    sqldb\n",
    "    ui\n",
    "    frontend.js\n",
    "    backend.js\n",
    "    frontend.flask\n",
    "    backend.flask\n",
    "\n",
    "You may use chat GPT to create sample outputs NOT THE LOGS. IE:\n",
    "\n",
    "    System failure\n",
    "    Database corruption\n",
    "    Disk failure detected\n",
    "    Database corruption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec9ba30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log files created \n"
     ]
    }
   ],
   "source": [
    "# Paste your python file here \n",
    "# don't forget to upload it with your submission\n",
    "import logging\n",
    "import random\n",
    "\n",
    "# Define different components and their sample messages    generated using chat\n",
    "components = {\n",
    "    \"frontend\": [\n",
    "        \"User navigated to homepage\",\n",
    "        \"Loaded dashboard components\",\n",
    "        \"User attempted invalid input on form\",\n",
    "        \"Failed to load user profile data\",\n",
    "        \"Frontend script crashed unexpectedly\"\n",
    "    ],\n",
    "    \"backend\": [\n",
    "        \"Received API request for user data\",\n",
    "        \"Slow response detected on API endpoint\",\n",
    "        \"Failed to process payment request\",\n",
    "        \"Backend server crash: out of memory\",\n",
    "        \"Authentication token verified\"\n",
    "    ],\n",
    "    \"sqldb\": [\n",
    "        \"Database connection established\",\n",
    "        \"Query execution time exceeds threshold\",\n",
    "        \"Failed to insert new user record\",\n",
    "        \"Database corruption detected in users table\",\n",
    "        \"Backup completed successfully\"\n",
    "    ],\n",
    "    \"authserver\": [\n",
    "        \"User login request received\",\n",
    "        \"Multiple failed login attempts detected\",\n",
    "        \"Session creation failed\",\n",
    "        \"Authentication service unavailable\",\n",
    "        \"Password reset token generated\"\n",
    "    ],\n",
    "    \"system\": [\n",
    "        \"System maintenance scheduled\",\n",
    "        \"CPU usage exceeds 85%\",\n",
    "        \"Disk write failure in logging system\",\n",
    "        \"Kernel panic detected\",\n",
    "        \"System heartbeat OK\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Log levels random gen\n",
    "levels = [logging.INFO, logging.WARNING, logging.ERROR, logging.CRITICAL]\n",
    "\n",
    "#  function to configure and write logs\n",
    "def setup_logger(component_name):\n",
    "    logger = logging.getLogger(component_name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    #handler saving the output next to the script file, you should see an output in the same folder the script is in, <<<<\n",
    "    handler = logging.FileHandler(f\"{component_name}.log\")\n",
    "    formatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "#  loggers for each component\n",
    "for component, messages in components.items():\n",
    "    logger = setup_logger(component)\n",
    "    \n",
    "    for _ in range(5):  # log entries fromatting liek the examples showed with time and all \n",
    "        message = random.choice(messages)\n",
    "        level = random.choice(levels)\n",
    "        \n",
    "        if level == logging.INFO:\n",
    "            logger.info(message)\n",
    "        elif level == logging.WARNING:\n",
    "            logger.warning(message)\n",
    "        elif level == logging.ERROR:\n",
    "            logger.error(message)\n",
    "        elif level == logging.CRITICAL:\n",
    "            logger.critical(message)\n",
    "\n",
    "print(\"Log files created \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5255ab",
   "metadata": {},
   "source": [
    "\n",
    "### Example Log Format\n",
    "\n",
    "You will work with logs that follow this simplified structure:\n",
    "\n",
    "```\n",
    "2025-04-11 23:20:36,913 | my_app | INFO | Request completed\n",
    "2025-04-11 23:20:36,914 | my_app.utils | ERROR | Unhandled exception\n",
    "2025-04-11 23:20:36,914 | my_app.utils.db | CRITICAL | Disk failure detected\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3659dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just one of the example out puts for authserver.log\n",
    "\n",
    "\n",
    "#2025-04-27 20:28:30,376 | authserver | ERROR | Session creation failed\n",
    "#2025-04-27 20:28:30,376 | authserver | WARNING | Multiple failed login attempts detected\n",
    "#2025-04-27 20:28:30,376 | authserver | ERROR | Session creation failed\n",
    "#2025-04-27 20:28:30,376 | authserver | ERROR | Password reset token generated\n",
    "#2025-04-27 20:28:30,377 | authserver | WARNING | Multiple failed login attempts detected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f6e84",
   "metadata": {},
   "source": [
    "## Part 2: Logging the Log File (40%)\n",
    "    New File\n",
    "### Part 2a: Read the Log File (see lab 7) (10%)\n",
    "\n",
    "\n",
    "Write a function to read the contents of a log file into a list of lines. Handle file errors gracefully.\n",
    "\n",
    "### Part 2b: Parse Log Lines (see code below if you get stuck) (10%)\n",
    "\n",
    "Use a regular expression to extract:\n",
    "- Timestamp\n",
    "- Log name\n",
    "- Log level\n",
    "- Message\n",
    "\n",
    "### Part 2c: Count Log Levels (20%)\n",
    "\n",
    "Create a function to count how many times each log level appears. Store the results in a dictionary. Then output it as a Json File\n",
    "You may pick your own format but here is an example. \n",
    "```python\n",
    "{\n",
    "    \"INFO\": \n",
    "    {\n",
    "        \"Request completed\": 42, \n",
    "        \"Heartbeat OK\": 7\n",
    "    }\n",
    "\n",
    "    \"WARNING\":\n",
    "    {\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc631f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to Part2_combined_log_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Paste your python file here don't for get to upload it with your submission\n",
    "#Part 2 a-c allows you to select what log file to read, on this case one was  made it all 5 of them, and extracts the time info with the count too \n",
    "#ofcourse you can secify the log component by seelcting what file\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "def read_log_file(filename):\n",
    "    lines = []\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found.\")\n",
    "    except IOError as e:\n",
    "        print(f\"An I/O error occurred: {e}\")\n",
    "    return lines\n",
    "\n",
    "def parse_log_line(line):\n",
    "    pattern = r'^(.*?\\d{2,3}) \\| (.*?) \\| (.*?) \\| (.*)$'\n",
    "    match = re.match(pattern, line)\n",
    "    if match:\n",
    "        timestamp = match.group(1)\n",
    "        log_name = match.group(2)\n",
    "        log_level = match.group(3)\n",
    "        message = match.group(4)\n",
    "        return timestamp, log_name, log_level, message\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def count_log_levels(lines):\n",
    "    log_counts = {}\n",
    "\n",
    "    for line in lines:\n",
    "        parsed = parse_log_line(line.strip())\n",
    "        if parsed:\n",
    "            _, _, log_level, message = parsed\n",
    "\n",
    "            if log_level not in log_counts:\n",
    "                log_counts[log_level] = {}\n",
    "\n",
    "            if message not in log_counts[log_level]:\n",
    "                log_counts[log_level][message] = 1\n",
    "            else:\n",
    "                log_counts[log_level][message] += 1\n",
    "    return log_counts\n",
    "\n",
    "def save_as_json(data, filename):\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    print(f\"✅ Results saved to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read and combine all five log files\n",
    "    all_lines = []\n",
    "    log_files = ['frontend.log', 'backend.log', 'sqldb.log', 'authserver.log', 'system.log']\n",
    "\n",
    "    for file in log_files:\n",
    "        lines = read_log_file(file)\n",
    "        all_lines.extend(lines)\n",
    "\n",
    "    # Count across all logs\n",
    "    log_counts = count_log_levels(all_lines)\n",
    "\n",
    "    # Save combined results to a JSON file\n",
    "    save_as_json(log_counts, 'combined_log_summary.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8a0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your python file here \n",
    "# don't forget to upload it with your submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045c30f",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Generate Summary Report (40%)\n",
    "    New File\n",
    "### Step 3a (20%):\n",
    " Develop a function that continuously monitors your JSON file(s) and will print a real-time summary of log activity. It should keep count of the messages grouped by log level (INFO, WARNING, ERROR, CRITICAL) and display only the critical messages. (I.e. If new data comes in the summary will change and a new critical message will be printed)\n",
    " - note: do not reprocess the entire file on each update.  \n",
    "\n",
    "### Step 3a: Use a Matplotlib (Lecture 10) (20%)\n",
    "Develop a function that continuously monitors your JSON file(s) and will graph in real-time a bar or pie plot of each of the errors.  (a graph for each log level). \n",
    "- The graph should show the distribution of log messages by level  (INFO, WARNING, ERROR, CRITICAL)  \n",
    "\n",
    "\n",
    "### Critical notes:\n",
    "- Your code mus use Daemon Threads (Lecture 14)\n",
    "- 3a and 3b do not need to run at the same time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea4429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your python file here \n",
    "# don't forget to upload it with your submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26eb058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a sample regex that parses a log file and extracts relevant information. \n",
    "# you will need to modify it. Review Lecture 11\n",
    "import re\n",
    "\n",
    "def parse_log_line(line):\n",
    "    pattern = r\"^(.*?)\\s\\|\\s(\\w+)\\s\\|\\s(\\w+)\\s\\|\\s(.*)$\"\n",
    "    match = re.match(pattern, line)\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
